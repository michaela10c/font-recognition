{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainfile = pandas.read_csv('train_data.csv')\n",
    "Trainlabels = pandas.read_csv('train_labels.csv')\n",
    "Testfile = pandas.read_csv('test_data.csv')\n",
    "X_train = Trainfile.values\n",
    "Y_train = Trainlabels.values\n",
    "X_test = Testfile.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, filename, pr=False):\n",
    "    model.fit(X_train, Y_train.flatten())\n",
    "    if pr:\n",
    "        pr('training accuracy:', accuracy_score(model.predict(X_train), Y_train.flatten()))\n",
    "    Y_test_pred = model.predict(X_test).astype(object)\n",
    "    np.savetxt(filename, np.dstack((np.arange(1, Y_test_pred.size+1),Y_test_pred))[0],\"%d,%s\",header=\"ID,Font\",comments=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging with DTs (Random Forest)\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth = 100, min_samples_split = 3, min_samples_leaf = 2)\n",
    "clf = BaggingClassifier(base_estimator=tree, n_estimators=2048).fit(X_train, Y_train.flatten())\n",
    "train_and_test(clf, \"bagging_dt.csv\")\n",
    "\n",
    "# 1000 estimators, max_depth=10: 68.740% test accuracy \n",
    "\n",
    "# 30 estimators, max_depth=30: 84.209% test accuracy \n",
    "\n",
    "# 20 estimators, max_depth=50: 88.774% test accuracy \n",
    "\n",
    "# 20 estimators, max_depth = 100: 89.137% test accuracy \n",
    "# 50 estimators, max_depth = 100: 89.698% test accuracy \n",
    "# 300 estimators, max_depth = 100: 89.986% test accuracy \n",
    "# 2048 estimators, max_depth = 100: 90.157% test accuracy <- Best\n",
    "\n",
    "# 500 estimators, max_depth = 150: 84.236% test accuracy \n",
    "\n",
    "# 20 estimators, max_depth=200: 89.295% test accuracy \n",
    "# 100 estimators, max_depth=200: 89.616% test accuracy \n",
    "\n",
    "# 100 estimators, max_depth=400: 89.883% test accuracy \n",
    "# 2048 estimators, max_depth=400: 90.109% test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier should give very similar overall performances (with same hyperparameters)\n",
    "# as to using Bagging with DTs (same thing!!)\n",
    "\n",
    "# Just tried some different # of estimators here\n",
    "train_and_test(RandomForestClassifier(n_estimators=256), \"random_forest.csv\")\n",
    "# gives 79.938% test accuracy with n=256 estimators \n",
    "\n",
    "train_and_test(RandomForestClassifier(n_estimators=256, max_depth=8), \"random_forest.csv\")\n",
    "# gives 58.110% test accuracy with n=256 estimators, max_depth=8 (UNDERFITTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extreme Random Forests (Extra-Trees Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(ExtraTreesClassifier(n_estimators=1000, max_depth=50, min_samples_split=2, random_state=0), \"extreme_random_forest.csv\")\n",
    "\n",
    "# 100 estimators, max_depth=200: 80.417% test accuracy\n",
    "# 1000 estimators, max_depth=100: 81.327% test accuracy\n",
    "# 1000 estimators, max_depth=50: 81.279% test accuracy\n",
    "# 100 estimators, max_depth=100: 80.417% test accuracy\n",
    "# 1000 estimators, max_depth=10: 60.047% test accuracy\n",
    "# 100 estimators, max_depth=10: 59.664% test accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging with 1-NN (k=1 in k-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging with 1-NN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "clf = BaggingClassifier(base_estimator=knn, n_estimators=300).fit(X_train, Y_train.flatten())\n",
    "Y_test_pred = clf.predict(X_test)\n",
    "np.savetxt(\"bagging_1nn_300.csv\", np.dstack((np.arange(1, Y_test_pred.size+1),Y_test_pred))[0],\"%d,%s\",header=\"ID,Font\",comments=\"\")\n",
    "\n",
    "# 300 estimators: 67% test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
